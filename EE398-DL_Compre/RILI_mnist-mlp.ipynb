{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions for converting from label to one hot encoding and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehotvector(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    onehotvector = np.zeros((len(labels), len(unique_labels)))\n",
    "    for index, label in enumerate(labels):\n",
    "        onehotvector[int(index), int(label)] = int(1)\n",
    "    return onehotvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotvectors_to_labels(onehotvectors):\n",
    "    labels = np.zeros(onehotvectors.shape[0])\n",
    "    labels = onehotvectors.argmax(axis=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define some functions for better readability in the later parts. First is the ReLU function for the hidden layer activations\n",
    "ReLU(z) = max(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a function for getting the gradient of the ReLU activations at the hidden layers\n",
    "d ReLU(Z)/dz = 1 if Z>0 ; 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_relu(Z):\n",
    "    return (Z>0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next is a function for the output layer activation: Softmax:\n",
    "softmax(Z) = exp(Z)/(SUM(exp(Z))\n",
    "\n",
    "To avoid python returning NaN due to exceeding of the max float64 value, we multiply a constant C to both the numerator and denominator. Setting the C to be the negative maximum value among all the softmax inputs, we have...\n",
    "\n",
    "stable_softmax(Z) = exp(Z+max(Z))/(SUM(exp(Z+max(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    exps = exps / np.sum(exps)\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a function for getting the gradient of the Softmax activation at the output layer, to be used in during backpropagation later on\n",
    "\n",
    "d softmax(Z)/dZ = Z - y\n",
    "\n",
    "where y = one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_softmax(softmax_out, onehot_true_labels):\n",
    "    return softmax_out - onehot_true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define a function for computing the categorical cross-entropy Loss of the whole network: L\n",
    "L = -SUM(y*log(y_pred))\n",
    "\n",
    "where **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer)\n",
    "\n",
    "***To save on computation time:***\n",
    "Since **y** are in one-hot encoded format, we can instead compute the -log(y_pred) only at the column indices that are '1' and then proceed to computing the summation. This is much faster compared to matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y):\n",
    "    #log_likelihood = -np.multiply(y,np.log(y_pred)) #too slow\n",
    "    log_likelihood = -np.log(y_pred[y.argmax()])\n",
    "    \n",
    "    #print('\\ty', np.transpose(y))\n",
    "    #print('y_pred', np.transpose(y_pred))\n",
    "    #print('log_likelihood', np.transpose(log_likelihood))\n",
    "    loss = np.sum(log_likelihood)\n",
    "                             \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and its corresponding gradient:\n",
    "dL/dy_pred = y_pred - y\n",
    "\n",
    "where once again, **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer)\n",
    "\n",
    "For computation speed, compute for y_pred - 1 only at the index where y=1 (e.g. y.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(y_pred, y):\n",
    "    y_pred[y.argmax()] = y_pred[y.argmax()] - 1\n",
    "    delta = y_pred\n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define a method for initializing the network weights and biases:\n",
    "For the initialization, we use the default setting of keras **Dense** class:\n",
    "\n",
    "Weight initialization: Glorot Uniform: (-6/sqrt(m+n) , 6/sqrt(m+n))\n",
    "where m=number of inputs, n=number of outputs\n",
    "\n",
    "Bias initialization: all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neurons(num_input, num_hidden1_neurons, num_hidden2_neurons, num_output):\n",
    "    #INPUT LAYER\n",
    "    x_in = np.zeros((num_input, 1))\n",
    "    init = 0.1\n",
    "    #HIDDEN LAYER 1: num_input inputs, num_hidden1_neurons outputs\n",
    "    #init = 6/np.sqrt(num_input+num_hidden1_neurons)\n",
    "    w_h1 = np.random.uniform(low=-init, high=init, size=(num_hidden1_neurons, num_input))\n",
    "    b_h1 = np.zeros([num_hidden1_neurons, 1])\n",
    "\n",
    "    #HIDDEN LAYER 2: num_hidden1_neurons inputs, num_hidden2_neurons outputs\n",
    "    #init = 6/np.sqrt(num_hidden1_neurons+num_hidden2_neurons)\n",
    "    w_h2 = np.random.uniform(low=-init, high=init, size=(num_hidden2_neurons, num_hidden1_neurons))\n",
    "    b_h2 = np.zeros([num_hidden2_neurons, 1])\n",
    "\n",
    "    #OUTPUT LAYER: num_hidden2_neurons inputs, num_output outputs\n",
    "    #init = 6/np.sqrt(num_hidden2_neurons+num_output)\n",
    "    w_out = np.random.uniform(low=-init, high=init, size=(num_output, num_hidden2_neurons))\n",
    "    b_out = np.random.uniform(low=-init, high=init, size=(num_output, 1))\n",
    "    \n",
    "    return x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also define a method for generating data and their corresponding labels for getting a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X, Y, batch_size):\n",
    "    length = X.shape[0]\n",
    "    for i in np.arange(0, length, batch_size):\n",
    "        yield (X[i:min(i+batch_size, length)], Y[i:min(i+batch_size, length)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for forward-pass yielding the prediction **y_pred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out):\n",
    "    ## HIDDEN LAYER 1\n",
    "    z_h1 = np.dot(w_h1,input_data) + b_h1\n",
    "    a_h1 = relu(z_h1)\n",
    "    #print(\"w_h1.shape\", w_h1.shape, \"input_data.shape\", input_data.shape, \"a_h1.shape\", a_h1.shape)\n",
    "    ## HIDDEN LAYER 2\n",
    "    z_h2 = np.dot(w_h2, a_h1) + b_h2\n",
    "    a_h2 = relu(z_h2)\n",
    "    #print(\"w_h2.shape\", w_h2.shape, \"a_h1.shape\", a_h1.shape, \"a_h2.shape\", a_h1.shape)\n",
    "    ## OUTPUT LAYER\n",
    "    z_out = np.dot(w_out, a_h2) + b_out\n",
    "    #print(\"\\n\\tz_out=\", np.transpose(z_out))\n",
    "    y_pred = softmax(z_out)\n",
    "    #print(\"\\ty_pred=\", np.transpose(y_pred))\n",
    "    #print(\"w_out.shape\", w_out.shape, \"a_h2.shape\", a_h2.shape, \"y_pred.shape\", y_pred.shape)\n",
    "    \n",
    "    return y_pred, a_h1, a_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch):\n",
    "    num_batches_processed = 0.0\n",
    "    y_preds = np.zeros(y_batch.shape)\n",
    "    # initialize the total error\n",
    "    average_batch_error = 0.0\n",
    "    total_batch_delta_out = 0.0\n",
    "    for train_index, x in enumerate(x_batch): #SINGLE-BATCH LOOP\n",
    "        x = x.reshape(-1, 1)\n",
    "        y = y_batch[train_index].reshape(-1,1)\n",
    "\n",
    "        ##### FORWARD PASS #####\n",
    "        y_pred, a_h1, a_h2 = predict(x, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "\n",
    "        ##### BATCH ERROR COMPUTATIONS ######\n",
    "        error = cross_entropy(y_pred, y)\n",
    "        #print('individual error:', error)\n",
    "        average_batch_error = average_batch_error + error\n",
    "        \n",
    "        total_batch_delta_out = total_batch_delta_out+ delta_cross_entropy(y_pred, y)\n",
    "\n",
    "        num_batches_processed += 1\n",
    "        \n",
    "        y_preds[train_index:] = np.transpose(y_pred) # collate the predictions\n",
    "        last_x = x # to be returned for parameter updates after batch gradient computations\n",
    "    \n",
    "    average_batch_error = average_batch_error/x_batch.shape[0]\n",
    "    total_batch_delta_out = total_batch_delta_out/x_batch.shape[0]\n",
    "    \n",
    "    return average_batch_error, total_batch_delta_out, num_batches_processed, last_x, y_preds, a_h1, a_h2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a method for learning rate scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(current_error, initial_LR):\n",
    "    LR = initial_LR\n",
    "    if current_error>0.006 and current_error<0.014:\n",
    "        LR = initial_LR/2\n",
    "    \n",
    "    return LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the weights and biases as globally shared parameters for ease of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h1, b_h1, w_h2, b_h2, w_out, b_out = np.zeros(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for the training the artificial neural network: ann_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output,\n",
    "            batch_size=128, LR=0.1, validation_period=10, validation_data=None, validation_labels=None):\n",
    "    global x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out \n",
    "    \n",
    "    # Initialize the input, output, and hidden layer neurons (i.e. their weights and biases matrices)\n",
    "    x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out = init_neurons(num_input, num_hidden1_neurons,\n",
    "                                                            num_hidden2_neurons, num_output)\n",
    "    # Remember the Initial LR for LR scheduling purposes:\n",
    "    initial_LR = LR\n",
    "    \n",
    "    MAX_EPOCH=1000\n",
    "    \n",
    "    # Generate a vector for the epoch numbers\n",
    "    epochs = range(0, MAX_EPOCH)\n",
    "    \n",
    "    # Stop training if the current total training error goes below this value\n",
    "    ERR_TERMINATION_COND = 0.001\n",
    "    \n",
    "    total_error = 0.0\n",
    "    num_batches_processed = 0.0\n",
    "    \n",
    "    for epoch_index in epochs:\n",
    "        print('\\n=========================================================\\nEPOCH # %d' % (epoch_index+1))\n",
    "        randomized_train_indices = np.random.permutation(train_data.shape[0])\n",
    "        randomized_x_train = np.take(x_train, randomized_train_indices, axis=0);\n",
    "        randomized_y_train = np.take(y_train, randomized_train_indices, axis=0);\n",
    "        for x_batch, y_batch in batch(randomized_x_train, randomized_y_train, batch_size):\n",
    "            #for train_index, x in enumerate(x_batch): #SINGLE-BATCH LOOP\n",
    "            #    x = x.reshape(-1, 1)\n",
    "            #    y = y_batch[train_index].reshape(-1,1)\n",
    "            #    \n",
    "            #    ##### FORWARD PASS #####\n",
    "            #    y_pred, a_h1, a_h2 = predict(x, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\n",
    "            #    \n",
    "            #    ##### BATCH ERROR COMPUTATIONS ######\n",
    "            #    total_batch_error = total_batch_error + cross_entropy(y_pred, y)\n",
    "            #    total_batch_delta_out = total_batch_delta_out+ delta_cross_entropy(y_pred, y)\n",
    "            #    \n",
    "            #    num_batches_processed += 1\n",
    "            \n",
    "            average_batch_error, total_batch_delta_out, temp_num_batches_processed, x, _, a_h1, a_h2 = predict_batch(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch)\n",
    "            total_error = total_error + average_batch_error\n",
    "            num_batches_processed += temp_num_batches_processed\n",
    "            \n",
    "            ##### BACK PROPAGATION (PERFORMED AT AFTER EACH BATCH PROCESSING #####\n",
    "            delta_h2 = delta_relu(a_h2)*(np.dot(np.transpose(w_out),total_batch_delta_out))\n",
    "            delta_h1 = delta_relu(a_h1)*(np.dot(np.transpose(w_h2), delta_h2))\n",
    "\n",
    "            ## Update the weights and biases\n",
    "            w_out = w_out - LR*total_batch_delta_out*np.transpose(a_h2)\n",
    "            b_out = b_out - LR*total_batch_delta_out\n",
    "\n",
    "            w_h2 = w_h2 - LR*delta_h2*np.transpose(a_h1)\n",
    "            b_h2 = b_h2 - LR*delta_h2\n",
    "\n",
    "            w_h1 = w_h1 - LR*delta_h1*np.transpose(x)\n",
    "            b_h1 = b_h1 - LR*delta_h1\n",
    "        total_average_error = total_error/num_batches_processed\n",
    "        print(\"\\ttotal average error: %f @ LR=%f\" % (total_error/num_batches_processed, LR))\n",
    "        LR = get_LR(total_average_error, initial_LR)\n",
    "        if( total_average_error < ERR_TERMINATION_COND):\n",
    "            print('TRAINING ERROR TARGET REACHED! STOPPING TRAINING...')\n",
    "            break\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and reshape the training and test data to 1x(28*28), then normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28)\n",
      "y_train shape:  (60000,)\n",
      "x_test shape:  (10000, 28, 28)\n",
      "y_test shape:  (10000,)\n",
      "new x_train shape after reshaping:  (60000, 784)\n",
      "new x_test shape after reshaping:  (10000, 784)\n",
      "new y_train shape after onehot vector encoding:  (60000, 10)\n",
      "new y_test shape after onehot vector encoding:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, x_train.shape[1]*x_train.shape[2]])\n",
    "x_train = x_train.astype('float64')/np.max(x_train)\n",
    "x_test = np.reshape(x_test, [-1, x_test.shape[1]*x_test.shape[2]])\n",
    "x_test = x_test.astype('float64')/np.max(x_test)\n",
    "\n",
    "print('new x_train shape after reshaping: ', x_train.shape)\n",
    "print('new x_test shape after reshaping: ', x_test.shape)\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = labels_to_onehotvector(y_train)\n",
    "y_test = labels_to_onehotvector(y_test)\n",
    "print('new y_train shape after onehot vector encoding: ', y_train.shape)\n",
    "print('new y_test shape after onehot vector encoding: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================\n",
      "EPOCH # 1\n",
      "\ttotal average error: 0.017414 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 2\n",
      "\ttotal average error: 0.017148 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 3\n",
      "\ttotal average error: 0.016892 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 4\n",
      "\ttotal average error: 0.016717 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 5\n",
      "\ttotal average error: 0.016389 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 6\n",
      "\ttotal average error: 0.016031 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 7\n",
      "\ttotal average error: 0.015628 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 8\n",
      "\ttotal average error: 0.015306 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 9\n",
      "\ttotal average error: 0.014976 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 10\n",
      "\ttotal average error: 0.014724 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 11\n",
      "\ttotal average error: 0.014508 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 12\n",
      "\ttotal average error: 0.014277 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 13\n",
      "\ttotal average error: 0.014077 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 14\n",
      "\ttotal average error: 0.013982 @ LR=0.200000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 15\n",
      "\ttotal average error: 0.013925 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 16\n",
      "\ttotal average error: 0.013862 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 17\n",
      "\ttotal average error: 0.013801 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 18\n",
      "\ttotal average error: 0.013793 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 19\n",
      "\ttotal average error: 0.013718 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 20\n",
      "\ttotal average error: 0.013570 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 21\n",
      "\ttotal average error: 0.013374 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 22\n",
      "\ttotal average error: 0.013183 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 23\n",
      "\ttotal average error: 0.013007 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 24\n",
      "\ttotal average error: 0.012818 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 25\n",
      "\ttotal average error: 0.012629 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 26\n",
      "\ttotal average error: 0.012460 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 27\n",
      "\ttotal average error: 0.012305 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 28\n",
      "\ttotal average error: 0.012178 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 29\n",
      "\ttotal average error: 0.012041 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 30\n",
      "\ttotal average error: 0.011927 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 31\n",
      "\ttotal average error: 0.011861 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 32\n",
      "\ttotal average error: 0.011765 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 33\n",
      "\ttotal average error: 0.011677 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 34\n",
      "\ttotal average error: 0.011586 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 35\n",
      "\ttotal average error: 0.011501 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 36\n",
      "\ttotal average error: 0.011434 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 37\n",
      "\ttotal average error: 0.011357 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 38\n",
      "\ttotal average error: 0.011270 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 39\n",
      "\ttotal average error: 0.011190 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 40\n",
      "\ttotal average error: 0.011110 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 41\n",
      "\ttotal average error: 0.011028 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 42\n",
      "\ttotal average error: 0.010939 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 43\n",
      "\ttotal average error: 0.010844 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 44\n",
      "\ttotal average error: 0.010761 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 45\n",
      "\ttotal average error: 0.010679 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 46\n",
      "\ttotal average error: 0.010601 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 47\n",
      "\ttotal average error: 0.010530 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 48\n",
      "\ttotal average error: 0.010455 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 49\n",
      "\ttotal average error: 0.010376 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 50\n",
      "\ttotal average error: 0.010305 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 51\n",
      "\ttotal average error: 0.010248 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 52\n",
      "\ttotal average error: 0.010188 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 53\n",
      "\ttotal average error: 0.010140 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 54\n",
      "\ttotal average error: 0.010086 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 55\n",
      "\ttotal average error: 0.010026 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 56\n",
      "\ttotal average error: 0.009965 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 57\n",
      "\ttotal average error: 0.009910 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 58\n",
      "\ttotal average error: 0.009851 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 59\n",
      "\ttotal average error: 0.009801 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 60\n",
      "\ttotal average error: 0.009750 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 61\n",
      "\ttotal average error: 0.009696 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 62\n",
      "\ttotal average error: 0.009656 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 63\n",
      "\ttotal average error: 0.009607 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 64\n",
      "\ttotal average error: 0.009548 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 65\n",
      "\ttotal average error: 0.009487 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 66\n",
      "\ttotal average error: 0.009432 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 67\n",
      "\ttotal average error: 0.009382 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 68\n",
      "\ttotal average error: 0.009334 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 69\n",
      "\ttotal average error: 0.009287 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 70\n",
      "\ttotal average error: 0.009240 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 71\n",
      "\ttotal average error: 0.009188 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttotal average error: 0.009138 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 73\n",
      "\ttotal average error: 0.009092 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 74\n",
      "\ttotal average error: 0.009051 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 75\n",
      "\ttotal average error: 0.009009 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 76\n",
      "\ttotal average error: 0.008963 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 77\n",
      "\ttotal average error: 0.008918 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 78\n",
      "\ttotal average error: 0.008872 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 79\n",
      "\ttotal average error: 0.008826 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 80\n",
      "\ttotal average error: 0.008784 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 81\n",
      "\ttotal average error: 0.008740 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 82\n",
      "\ttotal average error: 0.008695 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 83\n",
      "\ttotal average error: 0.008654 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 84\n",
      "\ttotal average error: 0.008616 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 85\n",
      "\ttotal average error: 0.008575 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 86\n",
      "\ttotal average error: 0.008534 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 87\n",
      "\ttotal average error: 0.008492 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 88\n",
      "\ttotal average error: 0.008452 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 89\n",
      "\ttotal average error: 0.008416 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 90\n",
      "\ttotal average error: 0.008383 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 91\n",
      "\ttotal average error: 0.008348 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 92\n",
      "\ttotal average error: 0.008310 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 93\n",
      "\ttotal average error: 0.008271 @ LR=0.100000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-6cfd4e5e7bb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m ann_fit(x_train, y_train, num_input=NUM_INPUT, num_hidden1_neurons=NUM_HIDDEN1_NEURONS,\n\u001b[0;32m     14\u001b[0m             \u001b[0mnum_hidden2_neurons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_HIDDEN2_NEURONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_OUTPUT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             batch_size=128, LR=LR)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-028e73b95071>\u001b[0m in \u001b[0;36mann_fit\u001b[1;34m(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output, batch_size, LR, validation_period, validation_data, validation_labels)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#    num_batches_processed += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0maverage_batch_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_batch_delta_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_num_batches_processed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_h2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mtotal_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_error\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maverage_batch_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mnum_batches_processed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_num_batches_processed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-02d9698481a5>\u001b[0m in \u001b[0;36mpredict_batch\u001b[1;34m(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m##### FORWARD PASS #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_h2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m##### BATCH ERROR COMPUTATIONS ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-8e9190c72acf>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(input_data, w_h1, b_h1, w_h2, b_h2, w_out, b_out)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m## HIDDEN LAYER 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mz_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_h1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_h1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0ma_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_h1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(\"w_h1.shape\", w_h1.shape, \"input_data.shape\", input_data.shape, \"a_h1.shape\", a_h1.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define number of neurons per layer\n",
    "NUM_INPUT = x_train.shape[1]\n",
    "NUM_HIDDEN1_NEURONS = 256\n",
    "NUM_HIDDEN2_NEURONS = 256\n",
    "NUM_OUTPUT = y_train.shape[1]\n",
    "\n",
    "# Initial Learning Rate of Keras SGD Optimizer: https://keras.io/api/optimizers/sgd/\n",
    "LR = 0.2\n",
    "\n",
    "# Define the maximum number of epochs to run before stopping even if the stopping criteria is not met\n",
    "MAX_EPOCH = 30000\n",
    "\n",
    "ann_fit(x_train, y_train, num_input=NUM_INPUT, num_hidden1_neurons=NUM_HIDDEN1_NEURONS,\n",
    "            num_hidden2_neurons=NUM_HIDDEN2_NEURONS, num_output=NUM_OUTPUT,\n",
    "            batch_size=128, LR=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_batch_error, _, _, _, y_pred, _, _ = predict_batch(x_test, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6325183847403039"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_batch_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([ 0.00025296,  0.00021894,  0.00048948,  0.00037947,  0.00038958,\n",
       "         0.00402058,  0.00067529, -0.01190333,  0.00014742,  0.0053296 ]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0], y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE TO SELF: up next is: dropout, figure out what's wrong with SGD (too sloooow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
