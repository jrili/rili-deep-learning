{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jessa\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions for converting from label to one hot encoding and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehotvector(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    onehotvector = np.zeros((len(labels), len(unique_labels)))\n",
    "    for index, label in enumerate(labels):\n",
    "        onehotvector[int(index), int(label)] = int(1)\n",
    "    return onehotvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotvectors_to_labels(onehotvectors):\n",
    "    labels = np.zeros(onehotvectors.shape[0])\n",
    "    labels = onehotvectors.argmax(axis=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define some functions for better readability in the later parts. First is the ReLU function for the hidden layer activations\n",
    "ReLU(z) = max(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a function for getting the gradient of the ReLU activations at the hidden layers\n",
    "d ReLU(Z)/dz = 1 if Z>0 ; 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_relu(Z):\n",
    "    return (Z>0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next is a function for the output layer activation: Softmax:\n",
    "softmax(Z) = exp(Z)/(SUM(exp(Z))\n",
    "\n",
    "To avoid python returning NaN due to exceeding of the max float64 value, we multiply a constant C to both the numerator and denominator. Setting the C to be the negative maximum value among all the softmax inputs, we have...\n",
    "\n",
    "stable_softmax(Z) = exp(Z+max(Z))/(SUM(exp(Z+max(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exps = np.exp(Z + np.max(Z))\n",
    "    exps = exps / np.sum(exps,axis=1).reshape(-1,1)\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define a function for computing the categorical cross-entropy Loss of the whole network: L\n",
    "L = -SUM(y*log(y_pred))\n",
    "\n",
    "where **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer).\n",
    "\n",
    "***To save on computation time:***\n",
    "Since **y** are in one-hot encoded format, we can instead compute the -log(y_pred) only at the column indices that are '1' and then proceed to computing the summation. This is much faster compared to matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y):\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    #log_likelihood = -np.multiply(y,np.log(y_pred)) #too slow\n",
    "    \n",
    "    # Get the negative log likelihoods of only the column(class) arg of y where y=1\n",
    "    log_likelihood = y_pred[range(batch_size), np.argmax(y, axis=1).reshape(1,-1)]\n",
    "    log_likelihood = -np.log(log_likelihood)\n",
    "    \n",
    "    batch_loss = np.sum(log_likelihood)/batch_size\n",
    "    \n",
    "    # Clip the categorical cross entropy loss at 20 (i.e. y_pred=0.000000001 when y=1) to prevent exploding gradients\n",
    "    LOSS_CLIP_VALUE = 20.0\n",
    "    batch_loss = min(batch_loss, LOSS_CLIP_VALUE)\n",
    "                             \n",
    "    #print('y', y[0:3])\n",
    "    #print('y_pred', y_pred[0:3])\n",
    "    #print(log_likelihood.shape,'log_likelihood', log_likelihood[0,0:3])\n",
    "    #print('batch_loss:', batch_loss)\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and its corresponding gradient:\n",
    "dL/dy_pred = y_pred - y\n",
    "\n",
    "where once again, **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer)\n",
    "\n",
    "For computation speed, compute for \"y_pred - 1\" only at the index where y=1 (e.g. y.argmax()) and leave the rest of y_pred unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(y_pred, y):\n",
    "    batch_size = y.shape[0]\n",
    "    y_pred[y.argmax()] = y_pred[y.argmax()] - 1\n",
    "    delta = y_pred/batch_size\n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define a method for initializing the network weights and biases:\n",
    "For the initialization, we use the default setting of keras **Dense** class:\n",
    "\n",
    "Weight initialization: Glorot Uniform: (-6/sqrt(m+n) , 6/sqrt(m+n))\n",
    "where m=number of inputs, n=number of outputs\n",
    "\n",
    "Bias initialization: all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neurons(num_input, num_hidden1_neurons, num_hidden2_neurons, num_output):\n",
    "    #INPUT LAYER\n",
    "    x_in = np.zeros((num_input, 1))\n",
    "    \n",
    "    #HIDDEN LAYER 1: num_input inputs, num_hidden1_neurons outputs\n",
    "    init = 6/np.sqrt(num_input+num_hidden1_neurons)\n",
    "    w_h1 = np.random.uniform(low=-init, high=init, size=(num_hidden1_neurons, num_input))\n",
    "    b_h1 = np.zeros([num_hidden1_neurons, 1])\n",
    "\n",
    "    #HIDDEN LAYER 2: num_hidden1_neurons inputs, num_hidden2_neurons outputs\n",
    "    init = 6/np.sqrt(num_hidden1_neurons+num_hidden2_neurons)\n",
    "    w_h2 = np.random.uniform(low=-init, high=init, size=(num_hidden2_neurons, num_hidden1_neurons))\n",
    "    b_h2 = np.zeros([num_hidden2_neurons, 1])\n",
    "\n",
    "    #OUTPUT LAYER: num_hidden2_neurons inputs, num_output outputs\n",
    "    init = 6/np.sqrt(num_hidden2_neurons+num_output)\n",
    "    w_out = np.random.uniform(low=-init, high=init, size=(num_output, num_hidden2_neurons))\n",
    "    b_out = np.random.uniform(low=-init, high=init, size=(num_output, 1))\n",
    "    \n",
    "    return x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also define a method for generating data and their corresponding labels for getting a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X, Y, batch_size):\n",
    "    length = X.shape[0]\n",
    "    for i in np.arange(0, length, batch_size):\n",
    "        yield (X[i:min(i+batch_size, length)], Y[i:min(i+batch_size, length)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for forward-pass yielding the prediction **y_pred**\n",
    "\n",
    "NOTE: dropout value means probability that a neuron will be **included** in the network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch, dropout=1.0):\n",
    "    #print(x_batch.shape, y_batch.shape)\n",
    "    #(128, 784) (128, 10)\n",
    "    \n",
    "    #print(w_h1.shape, b_h1.shape, w_h2.shape, b_h2.shape, w_out.shape, b_out.shape)\n",
    "    #     (256, 784) (256, 1)     (256, 256) (256, 1)    (10, 256)     (10, 1)\n",
    "    \n",
    "    batch_size = y_batch.shape[0]\n",
    "    ##### FORWARD PASS #####\n",
    "    A_h1 = relu(np.dot(x_batch, np.transpose(w_h1)) + np.transpose(b_h1))\n",
    "    #A_h1 = np.multiply(A_h1, np.random.choice([0,1], size=A_h1.shape, p=[(1-dropout), dropout]))\n",
    "    #print(A_h1.shape) #(128, 256)\n",
    "    A_h2 = relu(np.dot(A_h1, np.transpose(w_h2)) + np.transpose(b_h2))\n",
    "    #A_h2 = np.multiply(A_h2, np.random.choice([0,1], size=A_h2.shape, p=[(1-dropout), dropout]))\n",
    "    #print(A_h2.shape) #(128, 256)\n",
    "    y_pred = softmax(np.dot(A_h2, np.transpose(w_out)) + np.transpose(b_out))\n",
    "    #print(y_pred.shape) #(128, 10)\n",
    "    \n",
    "    ##### COMPUTE LOSS AND GRADIENT AT OUTPUT FOR BACKPROPAGATION USE #####\n",
    "    batch_loss = cross_entropy(y_pred, y_batch)\n",
    "    #print(batch_loss)\n",
    "    batch_delta_out = (1/batch_size)*np.sum(delta_cross_entropy(y_pred, y_batch), axis=0) #sum along the columns (expected shape is (10,1))\n",
    "    batch_delta_out = batch_delta_out.reshape(1,-1)\n",
    "    #print(\"batch_delta_out.shape:\", batch_delta_out.shape) #(128, 10)\n",
    "    \n",
    "    return batch_loss, y_pred, A_h1, A_h2, batch_delta_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a method for learning rate scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(current_error, initial_LR):\n",
    "    LR = initial_LR\n",
    "    #if current_error>2.316 and current_error<2.345:\n",
    "    if current_error>3.405 and current_error<3.76:\n",
    "        LR = initial_LR/2\n",
    "    elif current_error>3.22 and current_error<3.405:\n",
    "        LR = initial_LR/3\n",
    "    elif current_error>3.0 and current_error<3.22:\n",
    "        LR = initial_LR/4\n",
    "    elif current_error>2.85 and current_error<3.0:\n",
    "        LR = initial_LR/5\n",
    "    elif current_error>2.752 and current_error<2.85:\n",
    "        LR = initial_LR/6\n",
    "    elif current_error>0 and current_error<2.752:\n",
    "        LR = initial_LR/7\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the weights and biases as globally shared parameters for ease of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h1, b_h1, w_h2, b_h2, w_out, b_out = np.zeros(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for the training the artificial neural network: ann_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output,\n",
    "            batch_size=128, LR=0.1, dropout=1.0):\n",
    "    global w_h1, b_h1, w_h2, b_h2, w_out, b_out \n",
    "    \n",
    "    # Initialize the input, output, and hidden layer neurons (i.e. their weights and biases matrices)\n",
    "    x_in, w_h1, b_h1, w_h2, b_h2, w_out, b_out = init_neurons(num_input, num_hidden1_neurons,\n",
    "                                                            num_hidden2_neurons, num_output)\n",
    "    # Remember the Initial LR for LR scheduling purposes:\n",
    "    initial_LR = LR\n",
    "    \n",
    "    MAX_EPOCH=500\n",
    "    \n",
    "    # Generate a vector for the epoch numbers\n",
    "    epochs = range(0, MAX_EPOCH)\n",
    "    \n",
    "    # Stop training if the current total training error goes below this value\n",
    "    ERR_TERMINATION_COND = -np.log(0.99) # i.e. cross entropy when y_pred = 0.999 and y = 1\n",
    "    \n",
    "    total_error = 0.0\n",
    "    batch_loss = 0.0\n",
    "    num_batches_processed = 0.0\n",
    "    \n",
    "    for epoch_index in epochs:\n",
    "        print('\\n=========================================================\\nEPOCH # %d' % (epoch_index+1))\n",
    "        randomized_train_indices = np.random.permutation(train_data.shape[0])\n",
    "        randomized_x_train = np.take(x_train, randomized_train_indices, axis=0);\n",
    "        randomized_y_train = np.take(y_train, randomized_train_indices, axis=0);\n",
    "        for x_batch, y_batch in batch(randomized_x_train, randomized_y_train, batch_size):\n",
    "            batch_size = y_batch.shape[0]\n",
    "            batch_loss, y_pred, A_h1, A_h2, batch_delta_out = predict_batch(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch, dropout=dropout)\n",
    "            \n",
    "            total_error = total_error + batch_loss\n",
    "            num_batches_processed += 1\n",
    "            \n",
    "            ##### BACK PROPAGATION (PERFORMED AT AFTER EACH BATCH PROCESSING) #####\n",
    "            \n",
    "            ### Compute batch gradients at each layer (Don't forget to divide by batch_size!)\n",
    "            #print(A_h2.shape, w_out.shape, batch_delta_out.shape)\n",
    "            #      (128, 256) (10, 256)     (1, 10)\n",
    "            delta_h2 = delta_relu(A_h2)*(np.dot(batch_delta_out, w_out))\n",
    "            delta_h2 = (np.sum(delta_h2, axis=0)/batch_size).reshape(1,-1)\n",
    "            #print(delta_h2.shape) #(1,256)\n",
    "            \n",
    "            #print(A_h1.shape, w_h2.shape, delta_h2.shape)\n",
    "            #     (128, 256)  (256, 256)    (1, 256)\n",
    "            delta_h1 = delta_relu(A_h1)*(np.dot(delta_h2, w_h2))\n",
    "            delta_h1 = (np.sum(delta_h2, axis=0)/batch_size).reshape(1,-1)\n",
    "            #print(delta_h1.shape) #(1,256)\n",
    "\n",
    "            ## Update the weights and biases\n",
    "            a_h2 = A_h2[-1,:].reshape(1,-1) # Get last output of layer 2 from the batch for parameter update purposes\n",
    "            #print(w_out.shape, batch_delta_out.shape, a_h2.shape)\n",
    "            #    (10, 256)     (1, 10)               (1, 256)\n",
    "            w_out = w_out + LR*np.dot(batch_delta_out.reshape(-1,1), a_h2)\n",
    "            b_out = b_out + LR*batch_delta_out.reshape(-1,1)\n",
    "\n",
    "            a_h1 = A_h1[-1,:].reshape(1,-1) # Get last output of layer 1 from the batchfor parameter update purposes\n",
    "            w_h2 = w_h2 - LR*np.dot(delta_h2.reshape(-1,1), a_h1)\n",
    "            b_h2 = b_h2 - LR*delta_h2.reshape(-1,1)\n",
    "\n",
    "            x = x_batch[-1,:].reshape(1,-1) # Get last input from batch for parameter update purposes\n",
    "            w_h1 = w_h1 - LR*np.dot(delta_h1.reshape(-1,1), x)\n",
    "            b_h1 = b_h1 - LR*delta_h1.reshape(-1,1)\n",
    "            \n",
    "            \n",
    "        total_average_error = total_error/num_batches_processed\n",
    "        print(\"\\ttotal average error: %f @ LR=%f\" % (total_error/num_batches_processed, LR))\n",
    "        print(\"\\tcurrent validation error: %f\" % (batch_loss))\n",
    "        LR = get_LR(total_average_error, initial_LR)\n",
    "        if( total_average_error < ERR_TERMINATION_COND):\n",
    "            print('TRAINING ERROR TARGET REACHED! STOPPING TRAINING...')\n",
    "            break\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and reshape the training and test data to 1x(28*28), then normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28)\n",
      "y_train shape:  (60000,)\n",
      "x_test shape:  (10000, 28, 28)\n",
      "y_test shape:  (10000,)\n",
      "new x_train shape after reshaping:  (60000, 784)\n",
      "new x_test shape after reshaping:  (10000, 784)\n",
      "new y_train shape after onehot vector encoding:  (60000, 10)\n",
      "new y_test shape after onehot vector encoding:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, x_train.shape[1]*x_train.shape[2]])\n",
    "x_train = x_train.astype('float64')/np.max(x_train)\n",
    "x_test = np.reshape(x_test, [-1, x_test.shape[1]*x_test.shape[2]])\n",
    "x_test = x_test.astype('float64')/np.max(x_test)\n",
    "\n",
    "print('new x_train shape after reshaping: ', x_train.shape)\n",
    "print('new x_test shape after reshaping: ', x_test.shape)\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = labels_to_onehotvector(y_train)\n",
    "y_test = labels_to_onehotvector(y_test)\n",
    "print('new y_train shape after onehot vector encoding: ', y_train.shape)\n",
    "print('new y_test shape after onehot vector encoding: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================\n",
      "EPOCH # 1\n",
      "\ttotal average error: 6.359850 @ LR=0.100000\n",
      "\tcurrent validation error: 8.534270\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 2\n",
      "\ttotal average error: 6.463504 @ LR=0.100000\n",
      "\tcurrent validation error: 3.470510\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 3\n",
      "\ttotal average error: 5.313887 @ LR=0.100000\n",
      "\tcurrent validation error: 2.803547\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 4\n",
      "\ttotal average error: 4.647262 @ LR=0.100000\n",
      "\tcurrent validation error: 2.731085\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 5\n",
      "\ttotal average error: 4.226903 @ LR=0.100000\n",
      "\tcurrent validation error: 2.754256\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 6\n",
      "\ttotal average error: 3.949715 @ LR=0.100000\n",
      "\tcurrent validation error: 2.641761\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 7\n",
      "\ttotal average error: 3.752294 @ LR=0.100000\n",
      "\tcurrent validation error: 2.655037\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 8\n",
      "\ttotal average error: 3.602591 @ LR=0.050000\n",
      "\tcurrent validation error: 2.541911\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 9\n",
      "\ttotal average error: 3.486538 @ LR=0.050000\n",
      "\tcurrent validation error: 2.645055\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 10\n",
      "\ttotal average error: 3.394375 @ LR=0.050000\n",
      "\tcurrent validation error: 2.531394\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 11\n",
      "\ttotal average error: 3.319744 @ LR=0.033333\n",
      "\tcurrent validation error: 2.623518\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 12\n",
      "\ttotal average error: 3.258250 @ LR=0.033333\n",
      "\tcurrent validation error: 2.519399\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 13\n",
      "\ttotal average error: 3.206907 @ LR=0.033333\n",
      "\tcurrent validation error: 2.588357\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 14\n",
      "\ttotal average error: 3.163529 @ LR=0.025000\n",
      "\tcurrent validation error: 2.677518\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 15\n",
      "\ttotal average error: 3.126485 @ LR=0.025000\n",
      "\tcurrent validation error: 2.779193\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 16\n",
      "\ttotal average error: 3.094616 @ LR=0.025000\n",
      "\tcurrent validation error: 2.669353\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 17\n",
      "\ttotal average error: 3.067049 @ LR=0.025000\n",
      "\tcurrent validation error: 2.789487\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 18\n",
      "\ttotal average error: 3.043104 @ LR=0.025000\n",
      "\tcurrent validation error: 2.537096\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 19\n",
      "\ttotal average error: 3.022254 @ LR=0.025000\n",
      "\tcurrent validation error: 2.820911\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 20\n",
      "\ttotal average error: 3.004071 @ LR=0.025000\n",
      "\tcurrent validation error: 2.648385\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 21\n",
      "\ttotal average error: 2.988225 @ LR=0.025000\n",
      "\tcurrent validation error: 2.846036\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 22\n",
      "\ttotal average error: 2.974372 @ LR=0.020000\n",
      "\tcurrent validation error: 2.617589\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 23\n",
      "\ttotal average error: 2.962229 @ LR=0.020000\n",
      "\tcurrent validation error: 2.591038\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 24\n",
      "\ttotal average error: 2.951615 @ LR=0.020000\n",
      "\tcurrent validation error: 2.812018\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 25\n",
      "\ttotal average error: 2.942375 @ LR=0.020000\n",
      "\tcurrent validation error: 2.679016\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 26\n",
      "\ttotal average error: 2.934385 @ LR=0.020000\n",
      "\tcurrent validation error: 2.584403\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 27\n",
      "\ttotal average error: 2.927542 @ LR=0.020000\n",
      "\tcurrent validation error: 2.725706\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 28\n",
      "\ttotal average error: 2.921759 @ LR=0.020000\n",
      "\tcurrent validation error: 2.777997\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 29\n",
      "\ttotal average error: 2.916959 @ LR=0.020000\n",
      "\tcurrent validation error: 2.836541\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 30\n",
      "\ttotal average error: 2.913082 @ LR=0.020000\n",
      "\tcurrent validation error: 2.769653\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 31\n",
      "\ttotal average error: 2.910079 @ LR=0.020000\n",
      "\tcurrent validation error: 2.727150\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 32\n",
      "\ttotal average error: 2.907907 @ LR=0.020000\n",
      "\tcurrent validation error: 2.928233\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 33\n",
      "\ttotal average error: 2.906538 @ LR=0.020000\n",
      "\tcurrent validation error: 2.811456\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 34\n",
      "\ttotal average error: 2.905940 @ LR=0.020000\n",
      "\tcurrent validation error: 2.731744\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 35\n",
      "\ttotal average error: 2.906094 @ LR=0.020000\n",
      "\tcurrent validation error: 2.812817\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 36\n",
      "\ttotal average error: 2.906985 @ LR=0.020000\n",
      "\tcurrent validation error: 2.881452\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 37\n",
      "\ttotal average error: 2.908600 @ LR=0.020000\n",
      "\tcurrent validation error: 3.029849\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 38\n",
      "\ttotal average error: 2.910932 @ LR=0.020000\n",
      "\tcurrent validation error: 2.976504\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 39\n",
      "\ttotal average error: 2.913977 @ LR=0.020000\n",
      "\tcurrent validation error: 2.822159\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 40\n",
      "\ttotal average error: 2.917733 @ LR=0.020000\n",
      "\tcurrent validation error: 3.019506\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 41\n",
      "\ttotal average error: 2.922204 @ LR=0.020000\n",
      "\tcurrent validation error: 3.044131\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 42\n",
      "\ttotal average error: 2.927392 @ LR=0.020000\n",
      "\tcurrent validation error: 3.211606\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 43\n",
      "\ttotal average error: 2.933301 @ LR=0.020000\n",
      "\tcurrent validation error: 3.250410\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 44\n",
      "\ttotal average error: 2.939939 @ LR=0.020000\n",
      "\tcurrent validation error: 3.402336\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 45\n",
      "\ttotal average error: 2.947311 @ LR=0.020000\n",
      "\tcurrent validation error: 3.280179\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 46\n",
      "\ttotal average error: 2.955425 @ LR=0.020000\n",
      "\tcurrent validation error: 3.403668\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 47\n",
      "\ttotal average error: 2.964287 @ LR=0.020000\n",
      "\tcurrent validation error: 3.394726\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 48\n",
      "\ttotal average error: 2.973905 @ LR=0.020000\n",
      "\tcurrent validation error: 3.467472\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 49\n",
      "\ttotal average error: 2.984292 @ LR=0.020000\n",
      "\tcurrent validation error: 3.359805\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 50\n",
      "\ttotal average error: 2.995460 @ LR=0.020000\n",
      "\tcurrent validation error: 3.619007\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 51\n",
      "\ttotal average error: 3.007428 @ LR=0.020000\n",
      "\tcurrent validation error: 3.730314\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 52\n",
      "\ttotal average error: 3.020374 @ LR=0.025000\n",
      "\tcurrent validation error: 3.884654\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 53\n",
      "\ttotal average error: 3.034477 @ LR=0.025000\n",
      "\tcurrent validation error: 3.886538\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 54\n",
      "\ttotal average error: 3.049687 @ LR=0.025000\n",
      "\tcurrent validation error: 3.845573\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttotal average error: 3.065922 @ LR=0.025000\n",
      "\tcurrent validation error: 4.238893\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 56\n",
      "\ttotal average error: 3.083101 @ LR=0.025000\n",
      "\tcurrent validation error: 4.046547\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 57\n",
      "\ttotal average error: 3.101297 @ LR=0.025000\n",
      "\tcurrent validation error: 4.178633\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 58\n",
      "\ttotal average error: 3.120732 @ LR=0.025000\n",
      "\tcurrent validation error: 4.437294\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 59\n",
      "\ttotal average error: 3.141678 @ LR=0.025000\n",
      "\tcurrent validation error: 4.404308\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 60\n",
      "\ttotal average error: 3.164257 @ LR=0.025000\n",
      "\tcurrent validation error: 4.621054\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 61\n",
      "\ttotal average error: 3.188323 @ LR=0.025000\n",
      "\tcurrent validation error: 4.809807\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 62\n",
      "\ttotal average error: 3.213621 @ LR=0.025000\n",
      "\tcurrent validation error: 4.501545\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 63\n",
      "\ttotal average error: 3.240087 @ LR=0.025000\n",
      "\tcurrent validation error: 4.934796\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 64\n",
      "\ttotal average error: 3.268422 @ LR=0.033333\n",
      "\tcurrent validation error: 5.002915\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 65\n",
      "\ttotal average error: 3.299012 @ LR=0.033333\n",
      "\tcurrent validation error: 5.259775\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 66\n",
      "\ttotal average error: 3.331519 @ LR=0.033333\n",
      "\tcurrent validation error: 5.115683\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 67\n",
      "\ttotal average error: 3.365783 @ LR=0.033333\n",
      "\tcurrent validation error: 5.799059\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 68\n",
      "\ttotal average error: 3.401741 @ LR=0.033333\n",
      "\tcurrent validation error: 5.611788\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 69\n",
      "\ttotal average error: 3.439339 @ LR=0.033333\n",
      "\tcurrent validation error: 6.179774\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 70\n",
      "\ttotal average error: 3.479187 @ LR=0.050000\n",
      "\tcurrent validation error: 6.253105\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 71\n",
      "\ttotal average error: 3.521766 @ LR=0.050000\n",
      "\tcurrent validation error: 6.653729\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 72\n",
      "\ttotal average error: 3.568366 @ LR=0.050000\n",
      "\tcurrent validation error: 7.243607\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 73\n",
      "\ttotal average error: 3.617724 @ LR=0.050000\n",
      "\tcurrent validation error: 7.556519\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 74\n",
      "\ttotal average error: 3.669026 @ LR=0.050000\n",
      "\tcurrent validation error: 7.265675\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 75\n",
      "\ttotal average error: 3.722782 @ LR=0.050000\n",
      "\tcurrent validation error: 7.529599\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 76\n",
      "\ttotal average error: 3.778967 @ LR=0.050000\n",
      "\tcurrent validation error: 8.092014\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 77\n",
      "\ttotal average error: 3.839421 @ LR=0.100000\n",
      "\tcurrent validation error: 8.967578\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 78\n",
      "\ttotal average error: 3.905908 @ LR=0.100000\n",
      "\tcurrent validation error: 9.607655\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 79\n",
      "\ttotal average error: 3.978203 @ LR=0.100000\n",
      "\tcurrent validation error: 9.629180\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 80\n",
      "\ttotal average error: 4.056077 @ LR=0.100000\n",
      "\tcurrent validation error: 10.977305\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 81\n",
      "\ttotal average error: 4.139322 @ LR=0.100000\n",
      "\tcurrent validation error: 11.338345\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 82\n",
      "\ttotal average error: 4.227762 @ LR=0.100000\n",
      "\tcurrent validation error: 10.978342\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 83\n",
      "\ttotal average error: 4.321258 @ LR=0.100000\n",
      "\tcurrent validation error: 12.292273\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 84\n",
      "\ttotal average error: 4.419629 @ LR=0.100000\n",
      "\tcurrent validation error: 13.409120\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 85\n",
      "\ttotal average error: 4.522680 @ LR=0.100000\n",
      "\tcurrent validation error: 13.378659\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 86\n",
      "\ttotal average error: 4.630235 @ LR=0.100000\n",
      "\tcurrent validation error: 14.561325\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 87\n",
      "\ttotal average error: 4.742109 @ LR=0.100000\n",
      "\tcurrent validation error: 15.014217\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 88\n",
      "\ttotal average error: 4.857887 @ LR=0.100000\n",
      "\tcurrent validation error: 15.340640\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 89\n",
      "\ttotal average error: 4.978737 @ LR=0.100000\n",
      "\tcurrent validation error: 15.950941\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 90\n",
      "\ttotal average error: 5.102858 @ LR=0.100000\n",
      "\tcurrent validation error: 16.043155\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 91\n",
      "\ttotal average error: 5.230737 @ LR=0.100000\n",
      "\tcurrent validation error: 17.593326\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 92\n",
      "\ttotal average error: 5.362272 @ LR=0.100000\n",
      "\tcurrent validation error: 17.752080\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 93\n",
      "\ttotal average error: 5.497350 @ LR=0.100000\n",
      "\tcurrent validation error: 16.980249\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 94\n",
      "\ttotal average error: 5.635857 @ LR=0.100000\n",
      "\tcurrent validation error: 18.496548\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 95\n",
      "\ttotal average error: 5.777534 @ LR=0.100000\n",
      "\tcurrent validation error: 19.663644\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 96\n",
      "\ttotal average error: 5.921272 @ LR=0.100000\n",
      "\tcurrent validation error: 19.890537\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 97\n",
      "\ttotal average error: 6.065009 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 98\n",
      "\ttotal average error: 6.207085 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 99\n",
      "\ttotal average error: 6.346687 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 100\n",
      "\ttotal average error: 6.483524 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 101\n",
      "\ttotal average error: 6.617682 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 102\n",
      "\ttotal average error: 6.749203 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 103\n",
      "\ttotal average error: 6.878164 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 104\n",
      "\ttotal average error: 7.004639 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 105\n",
      "\ttotal average error: 7.128700 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 106\n",
      "\ttotal average error: 7.250413 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 107\n",
      "\ttotal average error: 7.369847 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 108\n",
      "\ttotal average error: 7.487064 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttotal average error: 7.602125 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 110\n",
      "\ttotal average error: 7.715089 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 111\n",
      "\ttotal average error: 7.826013 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 112\n",
      "\ttotal average error: 7.934953 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 113\n",
      "\ttotal average error: 8.041959 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 114\n",
      "\ttotal average error: 8.147085 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 115\n",
      "\ttotal average error: 8.250378 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 116\n",
      "\ttotal average error: 8.351886 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 117\n",
      "\ttotal average error: 8.451655 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 118\n",
      "\ttotal average error: 8.549730 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 119\n",
      "\ttotal average error: 8.646153 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 120\n",
      "\ttotal average error: 8.740966 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 121\n",
      "\ttotal average error: 8.834208 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 122\n",
      "\ttotal average error: 8.925918 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 123\n",
      "\ttotal average error: 9.016135 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 124\n",
      "\ttotal average error: 9.104893 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 125\n",
      "\ttotal average error: 9.192228 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 126\n",
      "\ttotal average error: 9.278174 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 127\n",
      "\ttotal average error: 9.362764 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 128\n",
      "\ttotal average error: 9.446030 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 129\n",
      "\ttotal average error: 9.528002 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 130\n",
      "\ttotal average error: 9.608711 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 131\n",
      "\ttotal average error: 9.688185 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 132\n",
      "\ttotal average error: 9.766453 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 133\n",
      "\ttotal average error: 9.843542 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 134\n",
      "\ttotal average error: 9.919478 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 135\n",
      "\ttotal average error: 9.994287 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 136\n",
      "\ttotal average error: 10.067993 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 137\n",
      "\ttotal average error: 10.140622 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 138\n",
      "\ttotal average error: 10.212196 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 139\n",
      "\ttotal average error: 10.282739 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 140\n",
      "\ttotal average error: 10.352272 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 141\n",
      "\ttotal average error: 10.420817 @ LR=0.100000\n",
      "\tcurrent validation error: 20.000000\n",
      "\n",
      "=========================================================\n",
      "EPOCH # 142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a59a1b0fe540>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m ann_fit(x_train, y_train, num_input=NUM_INPUT, num_hidden1_neurons=NUM_HIDDEN1_NEURONS,\n\u001b[0;32m     17\u001b[0m             \u001b[0mnum_hidden2_neurons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_HIDDEN2_NEURONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_OUTPUT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             batch_size=128, LR=LR, dropout=DROPOUT)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-c9e277f625e4>\u001b[0m in \u001b[0;36mann_fit\u001b[1;34m(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output, batch_size, LR, dropout)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get last input from batch for parameter update purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mw_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_h1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_h1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mb_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_h1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdelta_h1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define number of neurons per layer\n",
    "NUM_INPUT = x_train.shape[1]\n",
    "NUM_HIDDEN1_NEURONS = 256\n",
    "NUM_HIDDEN2_NEURONS = 256\n",
    "NUM_OUTPUT = y_train.shape[1]\n",
    "\n",
    "# Initial Learning Rate of Keras SGD Optimizer: https://keras.io/api/optimizers/sgd/\n",
    "LR = 0.1\n",
    "\n",
    "# Define the maximum number of epochs to run before stopping even if the stopping criteria is not met\n",
    "MAX_EPOCH = 30000\n",
    "\n",
    "# Define Dropout probability, i.e. probability that a hidden neuron will be INCLUDED in the training:\n",
    "DROPOUT = 0.45\n",
    "\n",
    "ann_fit(x_train, y_train, num_input=NUM_INPUT, num_hidden1_neurons=NUM_HIDDEN1_NEURONS,\n",
    "            num_hidden2_neurons=NUM_HIDDEN2_NEURONS, num_output=NUM_OUTPUT,\n",
    "            batch_size=128, LR=LR, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_batch_error, _, _, _, y_pred, _, _ = predict_batch(x_test, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_batch_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0], y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([0,1], size=(2,2), p=[1-0.45, 0.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
