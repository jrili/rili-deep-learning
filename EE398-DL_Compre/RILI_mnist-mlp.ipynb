{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jessa\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions for converting from label to one hot encoding and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_onehotvector(labels):\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    onehotvector = np.zeros((len(labels), len(unique_labels)))\n",
    "    for index, label in enumerate(labels):\n",
    "        onehotvector[int(index), int(label)] = int(1)\n",
    "    return onehotvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotvectors_to_labels(onehotvectors):\n",
    "    labels = np.zeros(onehotvectors.shape[0])\n",
    "    labels = onehotvectors.argmax(axis=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define some functions for better readability in the later parts. First is the ReLU function for the hidden layer activations\n",
    "ReLU(z) = max(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and a function for getting the gradient of the ReLU activations at the hidden layers\n",
    "d ReLU(Z)/dz = 1 if Z>0 ; 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_relu(Z):\n",
    "    return (Z>0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next is a function for the output layer activation: Softmax:\n",
    "softmax(Z) = exp(Z)/(SUM(exp(Z))\n",
    "\n",
    "To avoid python returning NaN due to exceeding of the max float value, we multiply a constant C to both the numerator and denominator.\n",
    "C*exp(Z)/C(SUM(exp(Z-max(Z)))\n",
    "\n",
    "Setting the C to be the e^(-max(Z)), we have...\n",
    "\n",
    "stable_softmax(Z) = exp(Z-max(Z))/(SUM(exp(Z-max(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exps = np.exp(Z - np.max(Z))\n",
    "    denominator = np.sum(exps,axis=1).reshape(-1,1)\n",
    "    exps = exps / denominator\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define a function for computing the categorical cross-entropy Loss of the whole network: L\n",
    "L = -SUM(y*log(y_pred))\n",
    "\n",
    "where **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer).\n",
    "\n",
    "***To save on computation time:***\n",
    "Since **y** are in one-hot encoded format, we can instead compute the -log(y_pred) only at the column indices that are '1' and then proceed to computing the summation. This is much faster compared to matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y):\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    #log_likelihood = -np.multiply(y,np.log(y_pred)) #too slow\n",
    "    \n",
    "    # Get the negative log likelihoods of only the column(class) arg of y where y=1\n",
    "    log_likelihood = y_pred[range(batch_size), np.argmax(y, axis=1).reshape(1,-1)]\n",
    "    log_likelihood = -np.log(log_likelihood)\n",
    "    \n",
    "    loss = np.sum(log_likelihood)/batch_size\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and its corresponding gradient:\n",
    "dJ/dz_out = d/dz_out(-ylog(softmax(z_out))) = y_pred - y\n",
    "\n",
    "where once again, **y** is the one-hot encoded true labels and **y_pred** is the prediction of the network (i.e. output of the softmax output activation layer)\n",
    "\n",
    "For computation speed, compute for \"y_pred - 1\" only at the index where y=1 (e.g. y.argmax()) and leave the rest of y_pred unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_cross_entropy(y_pred, y):\n",
    "    batch_size = y.shape[0]\n",
    "    delta = y_pred-y\n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we define a method for initializing the network weights and biases:\n",
    "For the initialization, we use the default setting of keras **Dense** class:\n",
    "\n",
    "Weight initialization: Glorot Uniform: (-6/sqrt(m+n) , 6/sqrt(m+n))\n",
    "where m=number of inputs, n=number of outputs\n",
    "\n",
    "Bias initialization: all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_neurons(num_input, num_hidden1_neurons, num_hidden2_neurons, num_output):   \n",
    "    #HIDDEN LAYER 1: num_input inputs, num_hidden1_neurons outputs\n",
    "    init = 6/np.sqrt(num_input+num_hidden1_neurons)\n",
    "    w_h1 = np.random.uniform(low=-init, high=init, size=(num_hidden1_neurons, num_input))\n",
    "    b_h1 = np.zeros([num_hidden1_neurons, 1])\n",
    "\n",
    "    #HIDDEN LAYER 2: num_hidden1_neurons inputs, num_hidden2_neurons outputs\n",
    "    init = 6/np.sqrt(num_hidden1_neurons+num_hidden2_neurons)\n",
    "    w_h2 = np.random.uniform(low=-init, high=init, size=(num_hidden2_neurons, num_hidden1_neurons))\n",
    "    b_h2 = np.zeros([num_hidden2_neurons, 1])\n",
    "\n",
    "    #OUTPUT LAYER: num_hidden2_neurons inputs, num_output outputs\n",
    "    init = 6/np.sqrt(num_hidden2_neurons+num_output)\n",
    "    w_out = np.random.uniform(low=-init, high=init, size=(num_output, num_hidden2_neurons))\n",
    "    b_out = np.zeros([num_output, 1])\n",
    "    \n",
    "    return  w_h1, b_h1, w_h2, b_h2, w_out, b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also define a method for generating data and their corresponding labels for getting a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X, Y, batch_size):\n",
    "    length = X.shape[0]\n",
    "    for i in np.arange(0, length, batch_size):\n",
    "        yield (X[i:min(i+batch_size, length)], Y[i:min(i+batch_size, length)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for calculating the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    acc = (np.argmax(y_pred, axis=1) == np.argmax(y, axis=1)).astype(int)\n",
    "    acc = np.average(acc)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for forward-pass yielding the prediction **y_pred**\n",
    "\n",
    "NOTE: dropout value means probability that a neuron will be **included** in the network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(x_batch, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_batch, dropout=1.0):\n",
    "    \n",
    "    ##### FORWARD PASS #####\n",
    "        ### LAYER 1 ###\n",
    "    Z_h1 = np.dot(x_batch, np.transpose(w_h1)) + np.transpose(b_h1)\n",
    "    A_h1 = relu(Z_h1)\n",
    "        #DROP-OUT:\n",
    "    A_h1 = np.multiply(A_h1, np.random.choice([0,1], size=A_h1.shape, p=[(1-dropout), dropout]))\n",
    "    \n",
    "        ### LAYER 2 ###\n",
    "    Z_h2 = np.dot(A_h1, np.transpose(w_h2)) + np.transpose(b_h2)\n",
    "    A_h2 = relu(Z_h2)\n",
    "        #DROP-OUT\n",
    "    A_h2 = np.multiply(A_h2, np.random.choice([0,1], size=A_h2.shape, p=[(1-dropout), dropout]))\n",
    "    \n",
    "        ### OUTPUT LAYER ###\n",
    "    Z_out = np.dot(A_h2, np.transpose(w_out)) + np.transpose(b_out)\n",
    "    y_pred = softmax(Z_out)\n",
    "    \n",
    "    ##### COMPUTE LOSS AND GRADIENT AT OUTPUT FOR BACKPROPAGATION USE #####\n",
    "    batch_loss = cross_entropy(y_pred, y_batch)\n",
    "    batch_delta_out = delta_cross_entropy(y_pred, y_batch) # dJ/Z_out\n",
    "    \n",
    "    acc = compute_accuracy(y_pred, y_batch)\n",
    "    \n",
    "    return batch_loss, acc, y_pred, A_h1, A_h2, Z_h1, Z_h2, batch_delta_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a method for learning rate scheduling:\n",
    "* current_step: number of batches processed so far\n",
    "* decay_steps: number of steps before multiplying initial_LR by decay_rate (higher decay_steps leads to slower decaying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(initial_LR, current_step, decay_rate=0.96, decay_steps=200):\n",
    "    MIN_LR = 0.001\n",
    "    LR = initial_LR * np.power(decay_rate, (current_step/decay_steps))\n",
    "    LR = max(LR, MIN_LR)\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for computing parameter updates given neuron gradients and neuron inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_update(LR, delta, neuron_input, batch_size):\n",
    "    gradient_w = np.dot(np.transpose(delta), neuron_input) # Gradient of Loss wrt w\n",
    "    gradient_w = gradient_w/batch_size                     # Averaged for the batch\n",
    "\n",
    "    w_update = LR*gradient_w\n",
    "\n",
    "    gradient_b = np.transpose(delta)                       # Gradient of Loss wrt b\n",
    "    gradient_b = np.sum(gradient_b,axis=1)/batch_size      # Averaged for the batch\n",
    "    gradient_b = gradient_b.reshape(-1,1)\n",
    "\n",
    "    b_update = LR*gradient_b\n",
    "\n",
    "    return w_update, b_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for train and validation set splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(x, y, validation_ratio=0.2):\n",
    "    num_data_points = y.shape[0]\n",
    "    split_index = int(validation_ratio*num_data_points)\n",
    "    \n",
    "    randomized_indices = np.random.permutation(num_data_points)\n",
    "    \n",
    "    randomized_x = np.take(x, randomized_indices, axis=0);\n",
    "    x_validation = randomized_x[0:split_index]\n",
    "    x_train = randomized_x[split_index:]\n",
    "    \n",
    "    randomized_y = np.take(y, randomized_indices, axis=0);\n",
    "    y_validation = randomized_y[0:split_index]\n",
    "    y_train = randomized_y[split_index:]\n",
    "    \n",
    "    return x_train, y_train, x_validation, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare the weights and biases as globally shared parameters for ease of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h1, b_h1, w_h2, b_h2, w_out, b_out = np.zeros(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a method for the training the artificial neural network: ann_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_fit(train_data, train_labels, num_input, num_hidden1_neurons, num_hidden2_neurons, num_output,\n",
    "            batch_size=128, LR=0.1, dropout=1.0, max_epoch=20):\n",
    "    global w_h1, b_h1, w_h2, b_h2, w_out, b_out \n",
    "    \n",
    "    # Initialize the input, output, and hidden layer neurons (i.e. their weights and biases matrices)\n",
    "    w_h1, b_h1, w_h2, b_h2, w_out, b_out = init_neurons(num_input, num_hidden1_neurons,\n",
    "                                                            num_hidden2_neurons, num_output)\n",
    "    # Remember the Initial LR for LR scheduling purposes:\n",
    "    initial_LR = LR\n",
    "    \n",
    "    # Generate a vector for the epoch numbers\n",
    "    epochs = range(0, max_epoch)\n",
    "    \n",
    "    # Stop training if the current total training error goes below this value\n",
    "    ERR_TERMINATION_COND = -np.log(0.999) # i.e. cross entropy when y_pred = 0.999 and y = 1\n",
    "    \n",
    "    total_error = 0.0\n",
    "    total_training_acc = 0.0\n",
    "    num_batches_processed = 0.0\n",
    "    \n",
    "    x_train, y_train, x_validation, y_validation = train_validation_split(train_data, train_labels)\n",
    "    print(\"x_train.shape:\", x_train.shape, \"y_train.shape:\", y_train.shape, \"x_validation.shape\", x_validation.shape, \"y_validation.shape:\", y_validation.shape)\n",
    "    \n",
    "    for epoch_index in epochs:\n",
    "        print('\\n============================================================================\\nEPOCH # %d' % (epoch_index+1))\n",
    "        randomized_train_indices = np.random.permutation(x_train.shape[0])\n",
    "        randomized_x_train = np.take(x_train, randomized_train_indices, axis=0);\n",
    "        randomized_y_train = np.take(y_train, randomized_train_indices, axis=0);\n",
    "        for x_batch, y_batch in batch(randomized_x_train, randomized_y_train, batch_size):\n",
    "            this_batch_size = y_batch.shape[0]\n",
    "            \n",
    "            ##### FORWARD PASS for TRAINING DATA SET\n",
    "            batch_loss, acc, y_pred, A_h1, A_h2, Z_h1, Z_h2, batch_delta_out = predict_batch(x_batch,\n",
    "                                                                                        w_h1, b_h1, w_h2,\n",
    "                                                                                        b_h2, w_out, b_out,\n",
    "                                                                                        y_batch, dropout=dropout)\n",
    "\n",
    "            total_error = total_error + batch_loss\n",
    "            total_training_acc = total_training_acc + acc\n",
    "            LR = get_LR(initial_LR, current_step=num_batches_processed)\n",
    "            num_batches_processed += 1\n",
    "            total_average_error = total_error/num_batches_processed            \n",
    "            print(\"\\ttotal average error: %f train_acc:%f @LR=%f\" % (total_average_error, acc, LR), end=\"\\r\")\n",
    "            \n",
    "            ##### BACK PROPAGATION (PERFORMED AT AFTER EACH BATCH PROCESSING) #####\n",
    "            \n",
    "            ### Compute batch gradients at each layer (Don't forget to divide by this_batch_size!)\n",
    "            delta_h2 = delta_relu(Z_h2)*(np.dot(batch_delta_out, w_out))\n",
    "            delta_h1 = delta_relu(Z_h1)*(np.dot(delta_h2, w_h2))\n",
    "\n",
    "            ### Update the weights and biases\n",
    "            w_out_update, b_out_update = get_param_update(LR, batch_delta_out, A_h2, this_batch_size)\n",
    "            w_out = w_out - w_out_update\n",
    "            b_out = b_out - b_out_update\n",
    "\n",
    "            w_h2_update, b_h2_update = get_param_update(LR, delta_h2, A_h1, this_batch_size)\n",
    "            w_h2 = w_h2 - w_h2_update\n",
    "            b_h2 = b_h2 - b_h2_update\n",
    "\n",
    "            w_h1_update, b_h1_update = get_param_update(LR, delta_h1, x_batch, this_batch_size)\n",
    "            w_h1 = w_h1 - w_h1_update\n",
    "            b_h1 = b_h1 - b_h1_update\n",
    "            \n",
    "        # Get validation accuracy at end of epoch\n",
    "        _, validation_acc, _, _, _, _, _, _ = predict_batch(x_validation, w_h1, b_h1, w_h2, b_h2, w_out, b_out,\n",
    "                                                                y_validation, dropout=1.0)\n",
    "        print(\"\\n\\tvalidation_acc:%f\" % (validation_acc))\n",
    "        \n",
    "        if( total_average_error < ERR_TERMINATION_COND):\n",
    "            print('TRAINING ERROR TARGET REACHED! STOPPING TRAINING...')\n",
    "            break\n",
    "    print('\\n========================= END OF TRAINING =========================\\n\\n')\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here for Code Review!\n",
    "### Load the data and reshape the training and test data to 1x(28*28), then normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28)\n",
      "y_train shape:  (60000,)\n",
      "x_test shape:  (10000, 28, 28)\n",
      "y_test shape:  (10000,)\n",
      "new x_train shape after reshaping:  (60000, 784)\n",
      "new x_test shape after reshaping:  (10000, 784)\n",
      "new y_train shape after onehot vector encoding:  (60000, 10)\n",
      "new y_test shape after onehot vector encoding:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, x_train.shape[1]*x_train.shape[2]])\n",
    "x_train = x_train.astype('float64')/np.max(x_train)\n",
    "x_test = np.reshape(x_test, [-1, x_test.shape[1]*x_test.shape[2]])\n",
    "x_test = x_test.astype('float64')/np.max(x_test)\n",
    "\n",
    "print('new x_train shape after reshaping: ', x_train.shape)\n",
    "print('new x_test shape after reshaping: ', x_test.shape)\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = labels_to_onehotvector(y_train)\n",
    "y_test = labels_to_onehotvector(y_test)\n",
    "print('new y_train shape after onehot vector encoding: ', y_train.shape)\n",
    "print('new y_test shape after onehot vector encoding: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop\n",
    "LAYER1:       256 neurons: w_h1(256x784), b_h1(256,1), activation:ReLU\n",
    "\n",
    "LAYER2:       256 neurons: w_h2(256x256), b_h2(256,1), activation:ReLU\n",
    "\n",
    "OUTPUT LAYER: 10 neurons: w_out(10x256), b_out(10,1), activation: Softmax\n",
    "\n",
    "COST FUNCTION: Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (48000, 784) y_train.shape: (48000, 10) x_validation.shape (12000, 784) y_validation.shape: (12000, 10)\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 1\n",
      "\ttotal average error: 1.152408 train_acc:0.757812 @LR=0.277951\n",
      "\tvalidation_acc:0.899000\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 2\n",
      "\ttotal average error: 1.052963 train_acc:0.812500 @LR=0.257470\n",
      "\tvalidation_acc:0.918417\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 3\n",
      "\ttotal average error: 1.032176 train_acc:0.828125 @LR=0.238498\n",
      "\tvalidation_acc:0.928250\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 4\n",
      "\ttotal average error: 1.027768 train_acc:0.820312 @LR=0.220925\n",
      "\tvalidation_acc:0.928750\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 5\n",
      "\ttotal average error: 1.038109 train_acc:0.820312 @LR=0.204646\n",
      "\tvalidation_acc:0.922750\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 6\n",
      "\ttotal average error: 1.051249 train_acc:0.820312 @LR=0.189566\n",
      "\tvalidation_acc:0.933667\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 7\n",
      "\ttotal average error: 1.063606 train_acc:0.890625 @LR=0.175598\n",
      "\tvalidation_acc:0.928750\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 8\n",
      "\ttotal average error: 1.072253 train_acc:0.859375 @LR=0.162659\n",
      "\tvalidation_acc:0.934833\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 9\n",
      "\ttotal average error: 1.074983 train_acc:0.882812 @LR=0.150674\n",
      "\tvalidation_acc:0.936583\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 10\n",
      "\ttotal average error: 1.079714 train_acc:0.804688 @LR=0.139571\n",
      "\tvalidation_acc:0.936583\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 11\n",
      "\ttotal average error: 1.083042 train_acc:0.867188 @LR=0.129287\n",
      "\tvalidation_acc:0.941000\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 12\n",
      "\ttotal average error: 1.082235 train_acc:0.867188 @LR=0.119760\n",
      "\tvalidation_acc:0.940833\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 13\n",
      "\ttotal average error: 1.085580 train_acc:0.835938 @LR=0.110936\n",
      "\tvalidation_acc:0.936417\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 14\n",
      "\ttotal average error: 1.083448 train_acc:0.828125 @LR=0.102761\n",
      "\tvalidation_acc:0.941833\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 15\n",
      "\ttotal average error: 1.079346 train_acc:0.890625 @LR=0.095189\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jessa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Jessa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tvalidation_acc:0.934583\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 16\n",
      "\ttotal average error: 1.073489 train_acc:0.867188 @LR=0.088175\n",
      "\tvalidation_acc:0.942750\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 17\n",
      "\ttotal average error: 1.070189 train_acc:0.812500 @LR=0.081678\n",
      "\tvalidation_acc:0.940000\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 18\n",
      "\ttotal average error: 1.063636 train_acc:0.820312 @LR=0.075660\n",
      "\tvalidation_acc:0.929667\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 19\n",
      "\ttotal average error: 1.058289 train_acc:0.929688 @LR=0.070085\n",
      "\tvalidation_acc:0.946500\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 20\n",
      "\ttotal average error: 1.050310 train_acc:0.882812 @LR=0.064920\n",
      "\tvalidation_acc:0.944333\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 21\n",
      "\ttotal average error: 1.040807 train_acc:0.914062 @LR=0.060137\n",
      "\tvalidation_acc:0.943000\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 22\n",
      "\ttotal average error: 1.029677 train_acc:0.898438 @LR=0.055706\n",
      "\tvalidation_acc:0.947667\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 23\n",
      "\ttotal average error: 1.017531 train_acc:0.820312 @LR=0.051601\n",
      "\tvalidation_acc:0.947083\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 24\n",
      "\ttotal average error: 1.006100 train_acc:0.867188 @LR=0.047799\n",
      "\tvalidation_acc:0.944833\n",
      "\n",
      "============================================================================\n",
      "EPOCH # 25\n",
      "\ttotal average error: 0.993825 train_acc:0.929688 @LR=0.044277\n",
      "\tvalidation_acc:0.944083\n",
      "\n",
      "========================= END OF TRAINING =========================\n",
      "\n",
      "\n",
      "Test Accuracy = 0.944400\n"
     ]
    }
   ],
   "source": [
    "# Define number of neurons per layer\n",
    "NUM_INPUT = x_train.shape[1]\n",
    "NUM_HIDDEN1_NEURONS = 256\n",
    "NUM_HIDDEN2_NEURONS = 256\n",
    "NUM_OUTPUT = y_train.shape[1]\n",
    "\n",
    "# Initial Learning Rate of Keras SGD Optimizer: https://keras.io/api/optimizers/sgd/\n",
    "LR = 0.3\n",
    "\n",
    "# Define the maximum number of epochs to run before stopping even if the stopping criteria is not met\n",
    "MAX_EPOCH = 25\n",
    "\n",
    "# Define Dropout probability, i.e. probability that a hidden neuron will be INCLUDED in the training:\n",
    "DROPOUT = 0.45\n",
    "\n",
    "# INITIALIZE NETWORK PARAMETERS (to verify that previously trained parameters is reset)\n",
    "w_h1, b_h1, w_h2, b_h2, w_out, b_out = np.zeros(6)\n",
    "\n",
    "# TRAIN NETWORK\n",
    "ann_fit(x_train, y_train, num_input=NUM_INPUT, num_hidden1_neurons=NUM_HIDDEN1_NEURONS,\n",
    "            num_hidden2_neurons=NUM_HIDDEN2_NEURONS, num_output=NUM_OUTPUT,\n",
    "            batch_size=128, LR=LR, dropout=DROPOUT, max_epoch=MAX_EPOCH)\n",
    "\n",
    "# TEST NETWORK\n",
    "_, test_acc, _, _, _, _, _, _ = predict_batch(x_test, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_test)\n",
    "print(\"Test Accuracy = %f\" % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Accuracy = 0.945300\n"
     ]
    }
   ],
   "source": [
    "_, test_acc, _, _, _, _, _, _ = predict_batch(x_test, w_h1, b_h1, w_h2, b_h2, w_out, b_out, y_test)\n",
    "print(\"\\tTest Accuracy = %f\" % (test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
